"""
Compare traditional Playwright tests with AI-powered approaches.
Shows the advantages of Stagehand and Browser Use for dynamic content.
"""

import asyncio
import time
from pathlib import Path
import subprocess
import json
from datetime import datetime

class TestComparison:
    """Compare different testing approaches."""
    
    def __init__(self):
        self.results = {
            "traditional_playwright": {},
            "stagehand": {},
            "browser_use": {}
        }
        
    async def run_traditional_playwright(self):
        """Run traditional Playwright tests and track failures."""
        print("üé≠ Running Traditional Playwright Tests...")
        start_time = time.time()
        
        try:
            # Run the existing E2E tests that have checkbox issues
            result = subprocess.run(
                ["python", "../test_e2e_all_features.py"],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            success = result.returncode == 0
            output = result.stdout + result.stderr
            
            # Count checkbox failures
            checkbox_failures = output.count("Timeout") + output.count("strict mode violation")
            
            self.results["traditional_playwright"] = {
                "success": success,
                "duration": time.time() - start_time,
                "checkbox_failures": checkbox_failures,
                "error_messages": [line for line in output.split('\n') if 'Error' in line or 'Failed' in line][:5],
                "notes": "Brittle selectors fail with Streamlit's dynamic rendering"
            }
            
        except subprocess.TimeoutExpired:
            self.results["traditional_playwright"] = {
                "success": False,
                "duration": 300,
                "checkbox_failures": "Timeout",
                "error_messages": ["Test timed out after 5 minutes"],
                "notes": "Tests too slow due to retry mechanisms"
            }
            
    def run_stagehand_tests(self):
        """Run Stagehand AI-powered tests."""
        print("\nü§ñ Running Stagehand AI Tests...")
        start_time = time.time()
        
        try:
            result = subprocess.run(
                ["npm", "run", "test:stagehand"],
                capture_output=True,
                text=True,
                timeout=180,
                cwd="."
            )
            
            success = result.returncode == 0
            output = result.stdout + result.stderr
            
            # Count successful natural language actions
            nl_successes = output.count("‚úÖ") + output.count("successfully")
            
            self.results["stagehand"] = {
                "success": success,
                "duration": time.time() - start_time,
                "natural_language_actions": nl_successes,
                "self_healing_attempts": output.count("Retrying with alternative approach"),
                "error_messages": [line for line in output.split('\n') if 'Error' in line][:5],
                "notes": "Self-healing automation adapts to DOM changes"
            }
            
        except subprocess.TimeoutExpired:
            self.results["stagehand"] = {
                "success": False,
                "duration": 180,
                "error_messages": ["Test timed out"],
                "notes": "Timeout - may need API keys configured"
            }
            
    async def run_browser_use_tests(self):
        """Run Browser Use tests."""
        print("\nüåê Running Browser Use AI Tests...")
        start_time = time.time()
        
        try:
            # Run Browser Use tests with asyncio
            proc = await asyncio.create_subprocess_exec(
                "python", "browser_use/test_business_logic.py",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=180)
            output = stdout.decode() + stderr.decode()
            
            success = proc.returncode == 0
            
            # Count parallel executions and validations
            parallel_tasks = output.count("Parallel Validation")
            adaptive_learnings = output.count("adaptive") + output.count("Adaptive")
            
            self.results["browser_use"] = {
                "success": success,
                "duration": time.time() - start_time,
                "parallel_validations": parallel_tasks,
                "adaptive_learnings": adaptive_learnings,
                "error_messages": [line for line in output.split('\n') if 'Error' in line][:5],
                "notes": "Parallel execution and adaptive learning for business logic"
            }
            
        except asyncio.TimeoutError:
            self.results["browser_use"] = {
                "success": False,
                "duration": 180,
                "error_messages": ["Test timed out"],
                "notes": "Timeout - may need API keys configured"
            }
            
    def generate_comparison_report(self):
        """Generate a detailed comparison report."""
        report = f"""
# AI-Powered Testing Comparison Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary

This report compares traditional Playwright testing with AI-powered alternatives (Stagehand and Browser Use) 
for testing the Media Plan to Raw Data Automation Streamlit application.

## Test Results Comparison

### 1. Traditional Playwright
- **Success**: {self.results['traditional_playwright'].get('success', 'N/A')}
- **Duration**: {self.results['traditional_playwright'].get('duration', 0):.2f}s
- **Checkbox Failures**: {self.results['traditional_playwright'].get('checkbox_failures', 'N/A')}
- **Key Issues**: {self.results['traditional_playwright'].get('notes', 'N/A')}

### 2. Stagehand (AI-Powered)
- **Success**: {self.results['stagehand'].get('success', 'N/A')}
- **Duration**: {self.results['stagehand'].get('duration', 0):.2f}s
- **Natural Language Actions**: {self.results['stagehand'].get('natural_language_actions', 0)}
- **Self-Healing Attempts**: {self.results['stagehand'].get('self_healing_attempts', 0)}
- **Key Advantage**: {self.results['stagehand'].get('notes', 'N/A')}

### 3. Browser Use (AI-Powered)
- **Success**: {self.results['browser_use'].get('success', 'N/A')}
- **Duration**: {self.results['browser_use'].get('duration', 0):.2f}s
- **Parallel Validations**: {self.results['browser_use'].get('parallel_validations', 0)}
- **Adaptive Learnings**: {self.results['browser_use'].get('adaptive_learnings', 0)}
- **Key Advantage**: {self.results['browser_use'].get('notes', 'N/A')}

## Key Findings

### Advantages of AI-Powered Testing

1. **Resilience to UI Changes**
   - Stagehand uses natural language commands that adapt to DOM changes
   - No brittle CSS selectors that break with Streamlit's dynamic rendering

2. **Business Logic Adaptation**
   - Browser Use can understand and adapt to business logic changes
   - Reduces false positives from legitimate requirement changes

3. **Parallel Execution**
   - Browser Use enables parallel scenario testing
   - Significantly faster for comprehensive test suites

4. **Self-Healing Capabilities**
   - AI-powered tools automatically find alternative approaches when initial attempts fail
   - Reduces test maintenance overhead

### Specific Improvements for Streamlit

1. **Checkbox Handling**
   - Traditional: Fails due to dynamic rendering and timing issues
   - AI-Powered: Uses natural language to find and interact with checkboxes reliably

2. **Dynamic Content**
   - Traditional: Requires complex waits and retry logic
   - AI-Powered: Understands page state and adapts automatically

3. **Validation Logic**
   - Traditional: Hard-coded assertions break with business changes
   - AI-Powered: Understands intent and validates accordingly

## Recommendations

1. **For Immediate Issues**: Implement Stagehand for UI automation
   - Solves checkbox interaction problems
   - Minimal code changes required
   - Natural language commands are more maintainable

2. **For Long-term Strategy**: Adopt Browser Use for business logic testing
   - Handles evolving requirements gracefully
   - Enables parallel testing for faster feedback
   - Reduces test maintenance burden

3. **Hybrid Approach**: Use both tools for different test types
   - Stagehand for UI interaction tests
   - Browser Use for business logic and integration tests
   - Traditional Playwright for simple, stable scenarios

## Implementation Next Steps

1. Configure API keys in .env file
2. Run setup script: `./setup.sh`
3. Migrate failing Playwright tests to Stagehand
4. Create Browser Use tests for complex business scenarios
5. Set up CI/CD pipeline with AI-powered tests

"""
        return report
    
    async def run_comparison(self):
        """Run all test approaches and compare results."""
        print("üî¨ Starting Test Approach Comparison...\n")
        
        # Run tests (some in parallel where possible)
        await self.run_traditional_playwright()
        self.run_stagehand_tests()
        await self.run_browser_use_tests()
        
        # Generate and save report
        report = self.generate_comparison_report()
        
        report_path = Path("comparison_report.md")
        report_path.write_text(report)
        
        print("\n" + "="*60)
        print(report)
        print("="*60)
        print(f"\nüìä Full report saved to: {report_path}")
        
        # Save raw results as JSON
        results_path = Path("test_results.json")
        results_path.write_text(json.dumps(self.results, indent=2))
        print(f"üìà Raw results saved to: {results_path}")


if __name__ == "__main__":
    comparison = TestComparison()
    asyncio.run(comparison.run_comparison())